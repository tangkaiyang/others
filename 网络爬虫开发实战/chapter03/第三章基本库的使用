第三章 基本库的使用
3.1使用urllib
Python内置的HTTP请求库,有如下4个模块:
# request:
# error
# parse
# robotparser
3.1.1 发送请求
urllib.request
1.urlopen()
urllib.request模块提供了最基本的构造HTTP请求的方法,利用它可以模拟浏览器的一个请求发起过程,同时它还带有处理
授权验证(authentication),重定向(redirection),浏览器Cookies以及其它内容
urllib_test.py
urlopen()函数的API:
urlopen(url, data=None, timeout=socket._GLOBAL_DEFAULT_TIMEOUT,
            *, cafile=None, capath=None, cadefault=False, context=None):
data:可选,如果要添加该参数,并且如果它时字节流编码格式的内容,即bytes类型,
则需要通过bytes()方法转化.另外,如果传递了这个参数,则它的请求方式就不再时GET方式,而是POST方式.

URI格式:http://user:pass@www.example.jp:80/dir/index.htm?uid=1#ch1
timeout:可以通过设置这个超时时间来控制一个网页如果长时间未响应,就跳过它的抓取.
其他参数:
context参数,必须时ssl.SSLContext类型,用来指定SSL设置
cafile和capath分别指定CA证书和它的路径,这个再请求HTTPS链接时会有用
cadedault参数现在已弃用默认为False
2.Request
如果需要再请求中加入Headers等信息,就可以利用Request类
request_test.py
Request的构造方法:
class urllib.request.Request(url, data=None, headers={}, origin_req_host=None, unverifiable=False, method=None)
# 第一个参数url用于请求URL,
# data如果要传,必须传bytes(字节流)类型的.如果它是字典,可以先用urllib.parse模块里的urlencode()编码
# headers参数是一个字典,它就是请求头,我们可以再构造请求时通过headers参数直接构造也可以通过调用请求实例的add_header()方法添加
  添加请求头最常用的用法就是通过修改User-Agent来伪装浏览器,默认的User-Agent时Python-urllib,我们可以通过修改它来伪装浏览器.
  比如要伪装火狐浏览器,可以设置为Mozilla/5.0 (Windows NT 10.0; …) Gecko/20100101 Firefox/65.0
# 第四个参数origin_req_host指的是请求方法的host名称或IP地址
# 第五个参数unverifiable表示这个请求是否是无法验证的,默认是False,意思就是说用户没有足够权限来选择接受这个请求的结果.
  例如:我们请求一个HTML文档中的图片,但是我们没有自动抓取图像的权限,这是unverifiable的值就是True.
# 第六个参数method是一个字符串,用来指示请求使用的方法,比如GET,POST和PUT等
request_test.py
3.高级用法
更高级的操作(Cookies处理,代理设置等)
Handler工具.可以把它理解为各种处理器,有专门处理登录验证的,有处理Cookies的,有处理代理设置的.利用他们,几乎可以做到HTTP请求所有的事情.
BaseHandler类,它是所有其他Handler的父类,它提供了最基本的方法,例如default_open(),protocol_request()等
# HTTPDefaultErrorHandler:用于处理HTTP响应错误,错误都会抛出HTTPError类型的异常
# HTTPRedirectHandler:用于处理重定向
# HTTPCookieProcessor:用于处理Cookies
# ProxyHandler:用于设置代理,默认代理为空
# HTTPPasswordMgr:用于管理密码,它维护了用户名和密码的表
# HTTPBasicAuthHandler:用于管理认真,如果一个链接打开时需要认证,那么可以用它来解决认证问题
另一个比较重要的类就是OpenerDirector,我们可以称之为Opener.urlopen()方法实际上就是urllib为我们提供的一个Opener
为什么要引入Opener,因为需要实现更高级的功能.之前使用的Request和urlopen()相当于类库为你封装好了极其常用的方法,利用他们可以完成
基本的请求,但是现在不一样了,我们需要实现更高级的功能,所以需要深入一层进行配置,使用更底层的实例来完成操作,所以这里用到了Opener.
Opener可以使用open()方法,返回的类型和urlopen()如出一辙.利用Handler来构建Opener.
# 验证:有些网站打开时会弹出提示框,直接提示你输入用户名和密码,验证成功后,才能查看页面,
auth_test.py
# 代理:在做爬虫的时候,免不了要使用代理,如果要添加代理,
proxy_test.py
# Cookies:Cookies的处理就需要相关的Handler了
cookies_test.py
3.1.3 异常处理
urllib的error模块定义了request模块产生的异常.如果出现了问题,request模块便会抛出error模块中定义的异常
1.URLError:继承自OSError类,是error异常模块的基类,有request模块生的异常都可以通过捕获这个类来处理.
reason属性,即返回错误的原因
reason_test.py
2.HTTPError是URLError的子类,专门用来处理HTTP请求错误,比如认证请求失败等.
# code:返回HTTP状态吗,比如404表示网页不存在,500表示服务器内部错误等
# reason:同父类一样,用于返回错误的原因
# headers:返回请求头
reason_test.py
有时候,reason属性返回的不一定是字符串,也可能是一个对象.
reason_test.py
3.1.3 解析链接
urllib库中的parse模块,它定义了处理URL的标准接口,例如实现URL各部分的抽取,合并以及链接转换.
他支持如下协议的URL处理:file,ftp,gopher,hdl,http,https,imap,mailto,mms,news,nntp,prospero,rsync,rstp,rtspu,sftp,sip,sips,
snews,svn,svn+ssh,telnet和wais.
1.urlparse():该方法可以实现URL的识别和分段,
urlparse_test.py
urlparse('http://www.baidu.com/index.html;user?id=5#comment'
返回ParseResult类型的对象,包含六部分,scheme,netloc,path,params,query和fragment
scheme(协议)://netloc(域名)/path(访问路径);params(参数)?query(查询条件,一般用作GET类型的URL)#fragment(锚点,用于直接定位页面内部的下拉位置)
urlparse的API用法:
urllib.urlparse(urlstring, scheme='', allow_fragment=True)
# urlstring:必填项,待解析的URL
# scheme:默认的协议(如http或https等).假如这个链接没有带协议信息,会将这个作为默认的协议
urlparse_test.py
# allow_fragments:即是否忽略fragment.如果被设置未False,fragment部分就会被忽略,会被解析未path,paras或query的一部分,而fragment为空
urlparse_test.py
ParseResult实际上是一个援助,我们可以利用索引顺序来获取,也可以用属性名获取
urlparse_test.py
2.urlunparse():urlparse对立方法.接受的参数是一个可迭代对象,但是它的长度必须是6,否则会抛出参数数量不足或过多的问题.
urlunparse_test.py
3.urlsplit():与urlparse()方法非常相似,不过它不再单独解析params这一部分,只返回5个结果.params会合并到path中
urlsplit_test.py
返回结果是SplitResult,它其实是一个元组类型,既可以使用属性获取值,也可以使用索引获取值
4.urlunsplit():与urlunparse()类似,它也是将链接各部分组合成完整链接的方法,传入的参数也是一个可迭代对象,例如列表,元组等,唯一的区别是长度必须为5
5.urljoin:有了urlunparse()和urlunsplit()方法,我们可以完成链接的合并,不过前提必须要持有特定长度的对象,链接的每一部分都要清晰分开
此外,生成链接还有另外一个方法urljoin()方法.可以提供一个base_url(基础链接)作为第一个参数,将新的链接作为第二个参数,该方法会
分析base_url的scheme,netloc和path这三个内容对新链接确实的部分进行补充,最后返回结果.
urljoin_test.py
base_url提供了三项内容scheme,netloc和path.如果这3想在新的链接里不存在,就予以补充;如果新的链接存在,就是用新的链接的部分.
而base_url中的params,query和fragment是不起作用的
通过urljoin()方法,我们可以轻松实现链接的解析,拼合与生成
6.urlencode():在构造GET请求参数的时候非常有用
urlencode_test.py
将字典参数,转换为URL的参数
7.parse_qs():反序列化.如果我们有一串GET请求参数,利用parse_qs()方法,就可以将他转回字典
parse_qs_test.py
8.parse_qsl():将参数转化为元组组成的列表,例如[('name', 'germey'), ('age', '22')]
结果是一个列表,列表中的每一个元素都是一个元组,元组的第一个内容是参数名,第二个内容是参数值
9.quote():该方法可以将内容转化为URL编码的格式.URL中带有中文参数时,有时可能会导致乱码的问题,
此时用这个方法可以将中文字符转化为URL编码
quote_test.py
10.unquote():进行URL解码
quote_test.py
3.1.4 分析Robots协议
利用urllib的robotparser模块,我们可以实现网站Robots协议的分析.
1.Robots协议
Robots协议也称作爬虫协议,机器人协议,它的全名叫做网络爬虫排除标准(Robots Exclusion Protocol),
用来告诉爬虫和搜索引擎哪些页面可以抓取,哪些不可以抓取.
它通常是一个叫作robots.txt的文本文件,一般放在网站的根目录下:
当搜索爬虫访问一个站点时,它首先会检查这个站点根目录下是否存在robots.txt文件,如果存在,搜索爬虫会根据其中定义的爬取范围来爬取
.如果没有找到这个文件,搜索爬虫便会访问所有可直接访问的页面.
robots.txt样例:
User-agent: *
Disallow: /
Allow: /public/
这实现了所有搜索爬虫只允许爬取public目录的功能,将上述内容保存成robots.txt文件,放在网站的根目录下,和网站的路口文件(比如
index.php,index.html和index.jsp等)放在一起.
User-agent描述搜索爬虫的名称,这里将其设置为*这代表该协议对任何爬虫有效
User-agent: Baiduspider这就代表我们设置的规则对百度爬虫是有效的.如果有多条User-agent记录,则就会有多个爬虫会受到爬取限制,但至少需要指定一条
Disallow指定了不允许爬取的目录,比如设置为/则代表不允许抓取所有页面
Allow一般和Disallow一起使用,一般不会单独使用,用来排除某些限制.设置为/public/则表示所有页面不允许抓取,但可以抓取public目录
禁止所有爬虫访问任何目录的代码:
User-agent: *
Disallow: /
允许所有爬虫访问任何目录的代码:
User-agent: *
Disallow:
直接把robots.txt文件留空也是可以的.
禁止所有爬虫访问网站某些目录的代码:
User-agent: *
Disallow: /private/
Disallow: /tmp/
只允许一个爬虫访问的代码
User-agent: WebCrawler
Disallow:
User-agent: *
Disallow: /
2.爬虫名称
爬虫名称            名称              网站
BaiduSpider         百度              www.baidu.com
Goolebot            谷歌              www.google.com
360Spider           360搜索           www.so.com
YodaoBot            有道              www.youdao.com
ia_archiver         Alexa             www.alexa.com
Scooter             altavista         www.altavista.com
3.robotparser
了解了Robots协议之后,我们就可以使用robotparser模块来解析robots.txt了.该模块提供了一个类RobotFileParser,
它可以根据某网站的robots.txt文件来判断一个爬取爬虫是否有权限来爬取这个网页.
该类用起来非常简单,只需要在构造方法里传入robots.txt的链接即可.
urllib.robotparser.RobotFileParser(url='')
声明时可以不传入,默认为空,最后再使用set_url()方法设置
几个常用方法:
# set_url():用来设置robots.txt文件的链接.如果再创建RobotFileParser对象时传入链接,那么就不需要再使用这个方法设置了.
# read():读取robots.txt文件并进行分析.注意,这个方法执行一个读取和分析操作,如果不调用这个方法,接下来的判断都会为False,所以一定记得
调用这个方法.这个方法不会返回任何内容,但是执行了读取操作
# parse():用来解析robots.txt文件,传入的参数是robots.txt某些行的内容,它会按照robots.txt的语法规则来分析这些内容
# can_fetch():该方法传入两个参数,第一个是User-agent,第二个是要抓取的URL.返回的内容是该搜索引擎是否可以抓取这个URL,返回结果是True或False
# mtime():返回的是上次抓取和分析robots.txt的事件,这对于长时间分析和抓取的搜索爬虫是很有必要的,你可能需要定期检查来抓去最新的robots.txt
# modified():它同样对长时间分析和抓取的搜索爬虫很有帮助,将当前时间设置为上次抓取和分析robots.txt的时间
robots_test.py
3.2使用requests:request_test.py
urllib的基本用法中有不方便的地方,比如处理网页验证和Cookies时,需要些Opener和Handler来处理.
为了更加方便地实现这些操作,就有了更为强大地库requests,
3.2.1 基本用法
1.安装requests库
2.实例引入
urllib库中的urlopen()方法实际上是以GET方式请求网页的,而requests中相应的方法就是get()方法,
这里我们调用的get()方法实现与urlopen()相同的操作,获得Response对象,然后分别输出了Response的类型(type),状态吗(status_code),
响应体的类型(type(response.text)),内容以及Cookies
通过运行结果可以发现,它的返回类型是requests.models.Rsponse,响应体的类型是字符串str,Cookies的类型是RequestCookieJar
一句话完成其他HTTP请求方式
r = requests.post('http://httpbin.org/post')
r = requests.put('http://httpbin.org/put')
r = requests.delete('http://httpbin.org/delete')
r = requests.head('http://httpbin.org/head')
r = requests.options('http://httpbin.org/options')
3.GET请求
# 基本实例
# 抓取网页
# 抓取二进制数据,网页实际是一个HTML文档.如果想抓取图片,音频,视频等文件.图片,音频,视频这些文件实际上都是由二进制码组成的,
有用有特定的保存格式和对应的解析方式,我们才可以看到这些形形色色的多媒体.
# 添加headers
4.POST请求
5.响应
3.2.2 高级用法
requests的一些高级用法,如文件上传,Cookies设置,代理设置等
1.文件上传
2.Cookies
3.会话维持
在requests中,如果直接利用get()或post()等方法的确可以做到模拟网页的请求,但是这实际上是相当不同的会话,
也就是说相当于你用了两个浏览器打开了不同的页面
场景:第一个请求利用post()方法登录,第二次想get()获取登陆后自己的个人信息
如果两个完全不相关的会话是不能获取个人信息的,
可以利用cookies,但是很繁琐
主要方法是维持同一个会话,也就是相当于打开一个新的浏览器选项卡而不是新开一个浏览器.但是又不想每次设置cookies,
这时候就可以使用Session对象
利用它,我们可以方便地维护一个会话,而且不用担心cookies地问题,它会帮我们自动处理好
4.SSL证书验证
requests提供了证书验证地功能.当发送HTTP请求地时候,它会检查SSL证书,我们可以使用verify参数控制是否检查此证书.
默认为True,会自动验证
