第九章代理的使用
反爬虫措施:比如,服务器会检测某个IP在单位时间内的请求次数，如果超过了某个阈值，那么服务器会直接拒绝服务，返回一些错误信息．
代理的基本知识及各种代理的使用方式,包括代理的设置,代理池的维护,付费代理的使用,ADSL拨号代理的搭建方法等内容
测试网址:http://httpbin.org/get

9.2 代理池的维护
网上有大量的免费代理,不能保证代理是可用的,
因为可能此IP被其他人使用来爬取同样的目标站点而被封禁,或者代理服务器突然发生故障或网络繁忙.
所以我们需要提前筛选,将不可用的代理剔除掉,保留可用代理.
搭建一个高效易用的代理池
1.Redis数据库并启动服务,安装aiohttp,requests,redis-py,pyquery,Flask库
2.代理池的目标:
基本模块为:存储模块,获取模块,检测模块,接口模块
# 存储模块:负责存储抓取下来的代理.首先要保证代理不重复,要标识代理的可用情况,还要动态实时处理每个代理,
所以一种比较高效和方便的存储方式就是使用Redis的SortedSet,即有序集合
# 获取模块:需要定时在各大代理网站抓取代理,代理可以是免费公开代理也可以是付费代理,代理的形式都是IP加端口,
此模块尽量从不同来源获取,尽量抓取高匿代理,抓取成功之后将可用代理保存到数据库中
# 检测模块:需要定时检测数据库中的代理.这里需要设置一个检测链接,最后是爬取哪个网站就检测哪个网站,这样更加有针对性,
如果要做一个通用型的代理,那可以设置百度等链接来检测.另外,我们需要标识每一个代理的状态,如设置分数标识,100分代表可用,
分数越少代表越不可用.检测一次,如果代理可用,我们可以将分数标志立即设置为100满分,也可以在原基础上加1分;如果代理不可用,
我们可以将分数标识减1分,当分数减到一定阈值后,代理就直接从数据库移除.通过这样的标识分数,我们就可以辨别代理的可用情况,
选用的时候会更有针对性.
# 接口模块:需要用API来提供对外服务的接口.其实我们可以直接连接数据库来取对应的数据,但是这样就需要知道数据库的连接信息,
并且要配置连接,而比较安全和方便的方式就是提供一个Web API接口,我们通过访问接口即可拿到可用代理.另外,由于可用代理可能
有多个,那么我们可以设置一个随机返回某个可用代理的接口,这样就能保证每个可用代理都可以取到,实现负载均衡
3.代理池的架构:
代理池分为4个模块:存储模块,获取模块,检测模块,接口模块
# 存储模块使用Redis的有序结合,用来做代理的去重和状态标识,同时它也是中心模块和基础模块,将其他模块串联起来
# 获取模块定时从代理网站获取代理,将获取的代理传递给存储模块,并保存到数据库
# 检测模块定时通过存储模块获取所有代理,并对代理进行检测,根据不同的检测结果对代理设置不同的标识
# 接口模块通过Web API提供服务接口,接口通过连接数据库并通过Web形式返回可用的代理
4.代理池的实现
# 存储模块:
使用Redis的有序集合,集合的每一个元素都是不重复的,对于代理池来说,集合的元素就编程一个个代理,也就是
IP加端口的形式,如60.207.237.111:8888,这样的一个代理就是集合的一个元素.另外,有序集合的每一个元素都有一个分数字段,
分数是可以重复的,可以是浮点数类型,也可以是整数类型.该集合会根据每一个元素的分数对集合进行排序,数值小的排在前面,数值大的
排在后面,这样就可以实现集合元素的排序了
对于代理池来说,这个分数可以作为判断一个代理是否可用的标志,100为最高分,代表最可用,0为最低分,代表最不可用.如果要获取可用代理,
可以从代理池中随机获取分数最高的代理,注意是随机,这样可以保证每个可用代理都会被调用到.
分数是我们判断代理稳定性的重要标准,设置分数规则如下所示
# 分数100为可用,检测器会定时循环检测每个代理可用情况,一旦检测到有可用的代理就立即设置100,检测到不可用就将分数减1,分数减至0后代理移除
# 新获取的代理的分数为10,如果测试可行,分数立即设置为100,不可行则分数减1,分数减至0后代理移除.
这只是一种解决方案,当然可能还有更合理的方案.之所以设置此方案有如下几个原因.
# 在检测到代理可用时,分数立即设置为100,这样可以保证所有代理有更大的机会被获取到.你可能会问,为什么不将分数jia1而是直接设
为最高100呢?设想一下,有的代理是从各大免费公开代理网站获取的,常常一个代理并没有那么稳定,平均5次请求可能有两次成功,3次
失败,如果按照这种方式来设置分数,那么这个代理几乎不可能别取到.如果想追求代理的稳定性,可以用上述方法,这种方法可确保分数最
高的代理一定是最稳定可用的.所以,我们这里采用"可用即设置100"的方法,确保可用的代理都可以被获取到
# 在检测到代理不可用时,分数减一,分数减到0,代理就移除,如果代理可用,分数就设置为100.由于很多代理是从免费网站获取的,所以新获取
的代理无效的比例非常高,可能不足10%.所以在这里我们将分数设置为10,检测的机会没有可用代理的100次那么多,也可以适当减少开销.
# 获取模块:crawler.py
获取模块的逻辑相对简单,首先要定义一个Crawler来从各大网站抓取代理
方便起见,我们将获取代理的每个方法统一定义为以crawl开头,这样扩展的时候只需要添加crawl开头的方法即可
在这里实现了几个示例,如抓取代理66,Proxy360(gg),Goubanjia(免费gg)三个代理网站,这些方法都定义成了生成器,
通过yield返回一个个代理.程序首先获取网页,然后用pyquery解析,解析出IP加端口的形式的代理然后返回.
然后定义了一个get_proxies()方法,将所有以crawl开头的方法调用一遍,获取每个方法返回的代理并组合成列表形式返回.
如何获取所有以crawl开头的方法名称呢,其实这里借助元类实现.
我们定义了ProxyMetaclass,Crawl类将它设置为元类,元类中实现了__new__()方法,这个方法有固定的几个参数,第四个参数attrs包含了类的
一些属性.我们可以遍历attrs这个参数即可获取类的所有方法信息,就像遍历字典一样,键名对应方法的名称.然后判断方法的开头是否crawl,
如果是,则将其加入到__CrawlFunc__属性中,这样我们就成功将所有以crawl开头的方法定义成了一个属性,动态获取到所有以crawl开头的方法
列表.
所有,如果要做扩展,我们需要添加一个以crawl开头的方法.例如抓取快代理,我们只需要在Crawler类中增加crawl_kuaidaili()方法,仿照其他几个
方法将其定义成生成器,抓取其网站的代理,然后通过yield返回代理即可.这样,我们可以非常方便的扩展,而不用关心类其他部分的实现逻辑
代理网站的添加非常灵活,不仅可以添加免费代理,也可以添加付费代理.一些付费代理的提取方式也类似,也是通过web的形式获取,然后进行解析.
解析方式可能更加简单,如解析纯文本或JSON,解析之后以同样的形式返回即可,再次不再代码实现,可以自行扩展
既然定义了Crawler类,接下来再定义一个Getter类,用来动态地调用所有以crawl开头地方法,然后获取抓取到地代理,将其加入到数据库存储起来
getter.py
Getter类就是获取器类,它定义了一个变量POOL_UPPER_THRESHOLD来表示代理池地最大数量,这个数量可以灵活配置,然后定义了is_over_threshold()
方法来判断代理池是否已经达到了容量阈值.is_over_threshold()方法调用了RedisClient地count()方法来获取代理地输入,然后进行判断.
如果数量达到了阈值,则返回True,否则返回False.如果不想加入这个限制,可以将此方法永久返回True
接下来定义run()方法,该方法首先判断了代理池是否达到阈值,然后在这里就调用了Crawler类地__CrawlerFunc__属性,获取到所有以crawl开头的
方法列表,一次通过get_proxies()方法调用,得到各个方法抓取到的代理,然后再利用RedisClient的add()方法加入数据库,这样获取模块的工作就完成了
# 检测模块
我们已经成功将各个网站的代理获取下来了,现在就需要一个检测魔窟啊来对所有代理进行多轮检测.代理检测可用,分数就设置为100,代理不可用,
分数减1,这样就可以实时改变每个代理的可用情况.如要获取有效代理只需要获取分数高的代理即可
有用代理的数量非常多,为了提高代理的检测效率,我们再这里使用异步请求库aiohttp来进行检测
request作为一个同步请求库,我们再发出一个请求之后,程序需要等待网页加载完成之后才能继续执行.也就是这个过程会阻塞等待响应,如果服务器
响应非常慢,比如一个请求等待十几秒,那么我们使用requests完成一个请求就会需要等待十几秒的时间,程序也不会继续往下执行,而在这十几秒的
时间里程序其实完全可以去做其他事情,比如调度其他的请求或者进行网页解析等
异步请求库就解决了这个问题,它类似JavaScript中的回调,即再请求发出之后,程序可以继续执行去做其他的事情,当响应到达时,程序再去处理这个
响应.于是,程序就没有被阻塞,可以充分利用时间和资源,大大提高效率
对应响应速度比较快的网站来说,requests同步请求和aiohttp异步请求的效果差距没那么大.可对于检测代理来说,检测一个代理一般需要十几秒
甚至几十秒的时间,这时候用aiohttp异步请求库的优势就大大体现出来了,效率可能会提高几十倍不止
tester.py
这里定义了一个类Tester,__init__()方法中建立了一个RedisClient对象,供该对象中其他方法使用.接下来定义了一个
test_single_proxy()方法,这个方法用来检测单个代理的可用情况,其参数就是被检测的代理.注意,test_single_proxy()方法前面加了async
关键字,这代表这个方法时异步的.方法内部首先创建了aiohttp的CleintSession对象,此对象类似于requests的Session对象,
可用直接调用该对象的get()方法来访问页面.在这里,代理的设置是通过proxy参数传递给get()方法,请求方法前面也需要加上async关键词来
标明其是异步请求,这也是aiohttp使用时的常见写法
测试的链接在这里定义为常量TEST_URL.如果针对某个网站有抓取需求,建议将TEST_URL设置为目标网站的地址,因为在抓取的过程中,代理本身可能
是有用的,但是该代理的IP已经被目标网站封掉了.
如果想做一个通用的代理池,则不许呀专门设置TEST_URL,可用将其设置为一个不会被封IP的网站,也可以设置为百度这类响应稳定的网站.
